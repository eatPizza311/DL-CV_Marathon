{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 知識點\n",
    "## RPN 的目的\n",
    "- 取代 Selective Search 提出 Region Proposal。\n",
    "- 目標是先提出可能包含物體的 proposals，因此一個訓練好的 RPN 應該要有很高的 Recall (預測為真且真的為真的比率很高)。\n",
    "\n",
    "> 相較於 Selective Search 是獨立作業，RPN的優點在於可放入模型中一起學習，因此能打造 End-to-End 的模型。 \n",
    "\n",
    "## RPN 詳解\n",
    "### 架構\n",
    "<img src=\"https://img-blog.csdn.net/20180304220940218\" width=600>\n",
    "上圖為 RPN 的基本結構，RPN 主要由兩條支線組成，上面那條用來預測前景或背景，下面那條則是用來預測BBOX 的 [x,y,w,h]。\n",
    "\n",
    "其中前景或背景支線的 18 與 BBOX 的支線的 36 分別帶表其深度，這是因為原文中作者在每個像素上設置了9個 default anchor boxes，每個 Box 要預測 [x,y,w,h] 與 [前景,背景]，因此 BBOX 支線就有 9×4=36 的深度，另一條則為 9×2=18 的深度：\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/375/1*JDQw0RwmnIKeRABw3ZDI7Q.png\">\n",
    "\n",
    "### 輸入、輸出\n",
    "而 RPN 的輸入為經過 CNN Backbone 所獲得的特徵圖，以及預設生成的 anchors (可想像成一堆預先設定的 Bounding Box)，輸出為 Bounding Box 的資訊以及前景、背景的信心程度。\n",
    "\n",
    "### 支線任務\n",
    "決定深度後，上方支線會學習每一個 Default BBOX 是前景還是背景，這個步驟主要是透過 Softmax 函數做分類。\n",
    "\n",
    "此階段為 RPN 的重點，透過『學習』讓模型能更精準地提出可能包含物件的『Region Proposal』，其中輸出預測框如與標註框的 IOU 大於一定的值我們就稱這個 BBOX 是前景 (原文選用 IOU>0.7)，藉此讓模型學習物件的特徵。\n",
    "\n",
    "輸出預測框由 Default BBOX 與輸出 [delta (x,y,w,h)] 轉換所獲得，而 RPN 的輸出資訊就是 BBOX 的 [x,y,w,h] 的偏移、縮放量 與 前後景 Score 的資訊。\n",
    "\n",
    "RPN 輸出前後景的 BBOX，會依照 score 排列，在訓練階段時，最後輸出 256 個 BBOX 給下一個階段(手動設置，可以變動)，128 個前景與 128 個背景，這麼做的原因是為了平衡前景、背景框數量，在預測時是輸出所有的 positive 的 proposals (前景)。\n",
    "\n",
    "簡化的 RPN 示意圖如下，Input 先經過 3*3 卷積，再通過兩條 1*1 卷積，其中 Input 為 CNN backbone 最後一層的 Feature map：\n",
    "<img src='https://cvdl-fileentity.cupoy.com/marathon/dailytask/1586226055109/1596512988929' width=300>\n",
    "\n",
    ">  Faster R-CNN 有很高的準確率與 RPN 關係相當大，RPN 這一步等於先幫 Proposal 做過一次BBOX regression，因此這個『Proposal』本身就已經有一定的準確率。\n",
    "\n",
    "## 如何設計Default Anchor Box\n",
    "\n",
    "1. 首先模型這邊要先設計好對應的值(Prediction)\n",
    "    - 透過 3×3 kernel ，Padding 為 1，以 Sliding window 的方式掃過整張圖，再用 1×1 kernel 壓到我們要的深度(18,36)。\n",
    "    - Output 值要與 Default Anchor Box 相對應，Faster R-CNN 原文在每一個 Pixel 上設計9個 Boxes，因此 Output 深度要等於『預測值 × Boxes 數量(9)』。\n",
    "    \n",
    "    \n",
    "2. 再來要在每個 Pixels 上設計 9 個 BBOX\n",
    "    - 九個 BBOX 分別為三種尺度，每個尺度有三種長寬比的 BBOX。\n",
    "\n",
    "Default Anchor Box 運用 Feature maps 上的 Pixels 當作中心點，並在每一個點上放入 9 個不同大小、長寬比例的 BBOX (下圖藍色框部分)，回放為原圖就像是下圖所展示的樣子。\n",
    "\n",
    "而回放原圖原理是透過 Feature Map 大小與原圖大小比例換算，舉例來說，原圖為 300×300，在 30×30 的 Feature Map 上畫了一個 1×1 (pixel) 的 BBOX，其對應回原圖就是一個 (10,10) 的 BBOX，兩者擁有相同的 Receptive Field。\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/700/1*XEHcNRvRybLzo66F6cn-jg.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 『本次練習內容』\n",
    "#### 學習搭建RPN層"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 『本次練習目的』\n",
    "  #### 了解Object Detection演算法中是如何做到分類又回歸BBOX座標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
    "\n",
    "input_shape = (1024, 1024, 3)\n",
    "img_input = Input(shape=input_shape)\n",
    "\n",
    "\"\"\"先使用一般CNN提取特徵\"\"\"\n",
    "def nn_base(img_input):\n",
    "    # block1\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    # 縮小1/2: 1024x1024 -> 512x512\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "    \n",
    "    # block2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    # 縮小1/2: 512x512 -> 256×256\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "    \n",
    "    # block3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    # 縮水1/2 256x256 -> 128x128\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "    \n",
    "    # block4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)    \n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    # 縮水1/2 128x128 -> 64x64\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "    \n",
    "    # block5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "\n",
    "    # 最後返回的x是 64x64x512 的feature map。\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" RPN \"\"\"\n",
    "def rpn(base_layers, num_anchors):\n",
    "    x = Conv2D(512, (3, 3), padding='same', activation='relu', \n",
    "               kernel_initializer='normal', name='rpn_conv1')(base_layers)\n",
    "    # rpn分類和迴歸\n",
    "    x_class = Conv2D(num_anchors*2, (1, 1), activation='softmax', name='rpn_out_class')(x)\n",
    "    x_reg = Conv2D(num_anchors*4, (1, 1), activation='linear', name='rpn_out_regress')(x)\n",
    "    return x_class, x_reg, base_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_layers = nn_base(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_class, x_reg, base_layers = rpn(base_layers, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification支線： Tensor(\"rpn_out_class/Identity:0\", shape=(None, 64, 64, 18), dtype=float32)\n",
      "BBOX Regression 支線： Tensor(\"rpn_out_regress/Identity:0\", shape=(None, 64, 64, 36), dtype=float32)\n",
      "CNN Output： Tensor(\"block5_conv3/Identity:0\", shape=(None, 64, 64, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print('Classification支線：',x_class) #'''確認深度是否為18'''\n",
    "print('BBOX Regression 支線：',x_reg) #'''確認深度是否為36'''\n",
    "print('CNN Output：',base_layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
